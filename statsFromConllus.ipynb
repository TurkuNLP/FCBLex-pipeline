{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import json\n",
    "import pyconll\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import openpyxl\n",
    "import seaborn as sns\n",
    "import math\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "JSON_PATH = \"Parsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other necessary variables\n",
    "books = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the conllus (jsons) as Dataframes\n",
    "\n",
    "for file in os.listdir(JSON_PATH):\n",
    "    #Opening json contents\n",
    "    with open(JSON_PATH+\"/\"+file) as json_file:\n",
    "        #Transform into dataframe\n",
    "        df = pd.read_json(json_file)\n",
    "        #Append as dict juuuuust in case we need the metadata\n",
    "        #Clip at 17 as the format for the filenames are standardized\n",
    "        books[file[:17]] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions which extract data from a dictionary to get data on sentences\n",
    "\n",
    "#Function that removes PUNCT\n",
    "def getNoPunct(sentences: dict) -> dict:\n",
    "    no_punct = {}\n",
    "    #Remove the rows from each dataframe what are classified as PUNCT\n",
    "    for key in sentences:\n",
    "        df = sentences[key]\n",
    "        no_punct[key] = df[df.upos != \"PUNCT\"]\n",
    "    return no_punct\n",
    "\n",
    "#Function that takes in a dictionary of [book_name, conllu_dataframe] and returns a dictionary with [book_name, sentences_dataframe]\n",
    "def getTokenData(books: dict) -> dict:\n",
    "    return_dict = {}\n",
    "    with tqdm(range(len(books.keys())), desc=\"Extracting sentences...\") as pbar:\n",
    "        #For key-value pair in dict\n",
    "        for key in books:\n",
    "            #Init a new array for sentences\n",
    "            sentence_dfs = []\n",
    "            df = books[key]\n",
    "            \n",
    "            #Only care about the sentences\n",
    "            for sentence in df['sentences']:\n",
    "                #Add dfs created from sentences to list\n",
    "                sentence_dfs.append(pd.DataFrame.from_dict(sentence['tokens']))\n",
    "            #Map book_name to a dataframe from all its sentences\n",
    "            sentece_df = pd.concat(sentence_dfs, ignore_index=True)\n",
    "            return_dict[key]=sentece_df\n",
    "            #Update pbar\n",
    "            pbar.update(1)\n",
    "    return return_dict\n",
    "\n",
    "#Function which returns a dictionary [book_name, lemma_freq_pivot_table]\n",
    "def getLemmaFrequencies(sentences: dict) -> dict:\n",
    "    lemma_freqs = {}\n",
    "    for key in sentences:\n",
    "        lemma_freqs[key] = getLemmaFreq(sentences[key])\n",
    "    return lemma_freqs\n",
    "\n",
    "#Return dataframe with lemmas in descending order (ignoring PUNCT and non alnums)\n",
    "def getLemmaFreq(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    #Get rid of PUNCT\n",
    "    no_punct = df[df.upos != \"PUNCT\"].copy()\n",
    "    #Make all into strings\n",
    "    no_punct['lemma'] = no_punct['lemma'].apply(lambda x: str(x))\n",
    "    #Remove non-alnums\n",
    "    no_punct['lemma'] = no_punct['lemma'].apply(lambda x: ''.join(filter(str.isalnum, x)))\n",
    "    #Filter rows with nothing in them\n",
    "    no_punct = no_punct[no_punct.text != '']\n",
    "    #Return a pivot_table turned DataFrame that counts the occurances of each lemma and sorts them in descending order\n",
    "    return pd.DataFrame.pivot_table(no_punct, columns='lemma', aggfunc='size').sort_values(ascending=False).reset_index().rename(columns={0: \"frequency\"})\n",
    "\n",
    "#Get frequencies of words (not PUNCT, all to lowercase, and removed all non alnums)\n",
    "def getWordFrequencies(sentences: dict) -> dict:\n",
    "    word_freqs = {}\n",
    "    for key in sentences:\n",
    "        df = sentences[key]\n",
    "        #Get rid of PUNCT\n",
    "        no_punct = df[df.upos != \"PUNCT\"].copy()\n",
    "        #Make words lowercase\n",
    "        no_punct['text'] = no_punct['text'].apply(lambda x: x.lower())\n",
    "        #Remove non-alnums\n",
    "        no_punct['text'] = no_punct['text'].apply(lambda x: ''.join(filter(str.isalnum, x)))\n",
    "        #Filter rows with nothing in them\n",
    "        no_punct = no_punct[no_punct.text != '']\n",
    "        #Map book_name to pivot table\n",
    "        word_freqs[key] = pd.DataFrame.pivot_table(no_punct, columns='text', aggfunc='size').sort_values(ascending=False).reset_index().rename(columns={0: \"frequency\"})\n",
    "    return word_freqs\n",
    "\n",
    "#Get amount of non-PUNCT tokens in sentences\n",
    "def getWordAmounts(sentences: dict) -> dict:\n",
    "    word_amounts = {}\n",
    "    for key in sentences:\n",
    "        df = sentences[key]\n",
    "        #Get rid of PUNCT\n",
    "        no_punct = df[df.upos != \"PUNCT\"]\n",
    "        word_amounts[key] = len(no_punct)\n",
    "    return word_amounts\n",
    "\n",
    "#Get PoS frequencies\n",
    "def getPOSFrequencies(sentences: dict) -> dict:\n",
    "    pos_freqs = {}\n",
    "\n",
    "    for key in sentences:\n",
    "        #Map book_name to pivot table\n",
    "        pos_freqs[key] = pd.DataFrame.pivot_table(sentences[key], columns='upos', aggfunc='size').sort_values(ascending=False).reset_index().rename(columns={0: \"frequency\"})\n",
    "\n",
    "    return pos_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to get metrics from sentences\n",
    "\n",
    "#Function the get the average length of the unique lemmas in the sentenes\n",
    "def getAvgLen(data: dict, column: str) -> dict:\n",
    "    avg_lens = {}\n",
    "    for key in data:\n",
    "        i = 1\n",
    "        total_len = 0\n",
    "        df = data[key]\n",
    "        #For each lemma count the length and add one to counter\n",
    "        for lemma in df[column]:\n",
    "            #Only care about strings\n",
    "            if type(lemma) is str:\n",
    "                total_len += len(lemma)\n",
    "                i += 1\n",
    "        #If no lemmas were found (should never happen but just in case), we make the avg_len be 0\n",
    "        if i==1:\n",
    "            avg_lens[key] = 0\n",
    "        else:\n",
    "            #Map book_name to avg lemma length\n",
    "            avg_lens[key] = total_len/(i-1.0)\n",
    "    return avg_lens\n",
    "\n",
    "#Function to calculate DP (deviation of proportions) of all the words in the corpus\n",
    "def getDP(v: dict, f_df: pd.DataFrame, s: dict) -> pd.DataFrame:\n",
    "\n",
    "    #First get the minimum s\n",
    "    min_s = 1\n",
    "    for key in s:\n",
    "        if s[key] < min_s:\n",
    "            min_s = s[key]\n",
    "    #For corpus parts that are length 1\n",
    "    if min_s == 1:\n",
    "        min_s = 0\n",
    "\n",
    "    v_series = {}\n",
    "    #Transform v into more usable form\n",
    "    for key in v:\n",
    "        v_df = v[key]\n",
    "        ser = v_df[v_df.columns[1]]\n",
    "        ser.index = v_df[v_df.columns[0]]\n",
    "        v_series[key] = ser\n",
    "    \n",
    "    texts = []\n",
    "    DP = []\n",
    "    DP_norm = []\n",
    "    with tqdm(range(len(f_df.index)), desc=\"DP calculations\") as pbar:\n",
    "\n",
    "        #Loop through every single word in the corpus\n",
    "        for k in range(len(f_df.index)):\n",
    "            #Get the freq of the word in the whole corpus\n",
    "            word = f_df.iloc[k, 0]\n",
    "            f = f_df.iloc[k, 1]\n",
    "            abs_sum = 0\n",
    "            #For each document in the corpus\n",
    "            for key in v_series:\n",
    "                #Freq of word in document. Set to 0 if not found\n",
    "                v_i = 0\n",
    "                try:\n",
    "                    v_i = v_series[key].loc[word]*1.0\n",
    "                except:\n",
    "                    v_i = 0.0\n",
    "                #Comparative size of document to whole corpus\n",
    "                s_i = s[key]\n",
    "                #Calculate the abs_sum used in calculating DP as written by Gries [2020]\n",
    "                abs_sum += abs(((v_i)/f)-s_i)\n",
    "            #Append word to list\n",
    "            texts.append(word)\n",
    "            #Calculate and append DP\n",
    "            dp = 0.5*abs_sum\n",
    "            DP.append(dp)\n",
    "            #Append DP_norm to list (alltho with how many documents we have, the normalization doesn't work very well at all)\n",
    "            DP_norm.append(dp/(1-min_s))\n",
    "            #Update pbar\n",
    "            pbar.update(1)\n",
    "    return pd.DataFrame({'text': texts, 'DP': DP, 'DP_norm': DP_norm})\n",
    "\n",
    "#Function to get contextual diversity\n",
    "def getCD(v: dict):\n",
    "    #Get number of books\n",
    "    books_num = len(v.keys())\n",
    "    word_series = []\n",
    "    #For each dataframe attached to a book, look for a frequency list and gather all the words in a list\n",
    "    for key in v:\n",
    "        v_df = v[key]\n",
    "        word_series.append(v_df[v_df.columns[0]])\n",
    "    #Add all words to a new dataframe\n",
    "    series = pd.concat(word_series, ignore_index=True)\n",
    "    #Create pivot table to count in how many books does a word appear in\n",
    "    CD_raw = series.value_counts()\n",
    "    #Return Contextual Diversity by dividing the number of appearances by the total number of books\n",
    "    return CD_raw.apply(lambda x: x/books_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to do with sub-corpora\n",
    "\n",
    "#Simple function to get sub_corpora from the whole package based on the target age group\n",
    "#Naming conventions are ISBN_age-group_register, where age-group is an int [1,3]\n",
    "def getSubCorp(corp: dict, num: int) -> dict:\n",
    "    sub_corp = {}\n",
    "    for key in corp:\n",
    "        if key.find('_'+str(num)+'_') != -1:\n",
    "            sub_corp[key] = corp[key]\n",
    "    return sub_corp\n",
    "\n",
    "#Combine sub-corp dicts into one dict\n",
    "def combineSubCorpDicts(corps: list) -> dict:\n",
    "    whole = corps[0].copy()\n",
    "    for i in range(1, len(corps)):\n",
    "        whole.update(corps[i])\n",
    "    return whole\n",
    "\n",
    "#Takes in a list of dataframes (or series) and combines them together\n",
    "def combineSubCorpsData(corps: list):\n",
    "    dfs = []\n",
    "    for df in corps:\n",
    "        dfs.append(df)\n",
    "    combined = pd.concat(dfs)\n",
    "    if type(combined) is pd.DataFrame:\n",
    "        return combined.groupby(combined.columns[0])[combined.columns[1]].sum().reset_index()\n",
    "    else:\n",
    "        return combined.groupby(level=0).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a4c14a5d9d44b5b9fc43f76ef22ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting sentences...:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x77dfb8b4a830>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tenojo/miniconda3/envs/Test/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "#Move to working with just sentence data\n",
    "#Whole corpus\n",
    "sentences = getTokenData(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subcorpora based on the target age groups\n",
    "sentences_1 = getSubCorp(sentences, 1)\n",
    "sentences_2 = getSubCorp(sentences, 2)\n",
    "sentences_3 = getSubCorp(sentences, 3)\n",
    "\n",
    "#Versions of sentences for more meaningful data\n",
    "sentences_no_punct_1 = getNoPunct(sentences_1)\n",
    "sentences_no_punct_2 = getNoPunct(sentences_2)\n",
    "sentences_no_punct_3 = getNoPunct(sentences_3)\n",
    "sentences_no_punct = combineSubCorpDicts([sentences_no_punct_1, sentences_no_punct_2, sentences_no_punct_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count lemma frequencies\n",
    "\n",
    "lemma_freqs_1 = getLemmaFrequencies(sentences_1)\n",
    "lemma_freqs_2 = getLemmaFrequencies(sentences_2)\n",
    "lemma_freqs_3 = getLemmaFrequencies(sentences_3)\n",
    "\n",
    "lemma_freqs = combineSubCorpDicts([lemma_freqs_1, lemma_freqs_2, lemma_freqs_3])\n",
    "\n",
    "#Count word frequencies\n",
    "\n",
    "word_freqs_1 = getWordFrequencies(sentences_1)\n",
    "word_freqs_2 = getWordFrequencies(sentences_2)\n",
    "word_freqs_3 = getWordFrequencies(sentences_3)\n",
    "\n",
    "word_freqs = combineSubCorpDicts([word_freqs_1, word_freqs_2, word_freqs_3])\n",
    "\n",
    "#Just for interest's sake, info on how many tokens (non-punct) are in each book\n",
    "\n",
    "word_amounts_1 = getWordAmounts(sentences_1)\n",
    "word_amounts_2 = getWordAmounts(sentences_2)\n",
    "word_amounts_3 = getWordAmounts(sentences_3)\n",
    "\n",
    "word_amounts = combineSubCorpDicts([word_amounts_1, word_amounts_2, word_amounts_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the average uniq lemma lengths\n",
    "avg_uniq_lemma_lens_1 = getAvgLen(lemma_freqs_1, 'lemma')\n",
    "avg_uniq_lemma_lens_2 = getAvgLen(lemma_freqs_2, 'lemma')\n",
    "avg_uniq_lemma_lens_3 = getAvgLen(lemma_freqs_3, 'lemma')\n",
    "avg_uniq_lemma_lens = getAvgLen(lemma_freqs, 'lemma')\n",
    "#print(avg_uniq_lemma_lens)\n",
    "\n",
    "#Count the average uniq word lengths\n",
    "avg_uniq_word_lens_1 = getAvgLen(word_freqs_1, 'text')\n",
    "avg_uniq_word_lens_2 = getAvgLen(word_freqs_2, 'text')\n",
    "avg_uniq_word_lens_3 = getAvgLen(word_freqs_3, 'text')\n",
    "avg_uniq_word_lens = getAvgLen(word_freqs, 'text')\n",
    "#print(avg_uniq_word_lens)\n",
    "\n",
    "#Count the average lemma lengths\n",
    "avg_lemma_lens_1 = getAvgLen(sentences_no_punct_1, 'lemma')\n",
    "avg_lemma_lens_2 = getAvgLen(sentences_no_punct_2, 'lemma')\n",
    "avg_lemma_lens_3 = getAvgLen(sentences_no_punct_3, 'lemma')\n",
    "avg_lemma_lens = getAvgLen(sentences_no_punct, 'lemma')\n",
    "#print(avg_lemma_lens)\n",
    "\n",
    "#Count the average word lengths\n",
    "avg_word_lens_1 = getAvgLen(sentences_no_punct_1, 'text')\n",
    "avg_word_lens_2 = getAvgLen(sentences_no_punct_2, 'text')\n",
    "avg_word_lens_3 = getAvgLen(sentences_no_punct_3, 'text')\n",
    "avg_word_lens = getAvgLen(sentences_no_punct, 'text')\n",
    "#print(avg_word_lens)\n",
    "\n",
    "\n",
    "#Combining results into dfs\n",
    "\n",
    "avg_uniq_lens_df_1 = pd.DataFrame.from_dict([avg_uniq_lemma_lens_1, avg_uniq_word_lens_1]).transpose().rename(columns={0: 'Unique lemmas avg length', 1: 'Unique words avg length'})\n",
    "avg_uniq_lens_df_2 = pd.DataFrame.from_dict([avg_uniq_lemma_lens_2, avg_uniq_word_lens_2]).transpose().rename(columns={0: 'Unique lemmas avg length', 1: 'Unique words avg length'})\n",
    "avg_uniq_lens_df_3 = pd.DataFrame.from_dict([avg_uniq_lemma_lens_3, avg_uniq_word_lens_3]).transpose().rename(columns={0: 'Unique lemmas avg length', 1: 'Unique words avg length'})\n",
    "avg_uniq_lens_df = pd.DataFrame.from_dict([avg_uniq_lemma_lens, avg_uniq_word_lens]).transpose().rename(columns={0: 'Unique lemmas avg length', 1: 'Unique words avg length'})\n",
    "\n",
    "\n",
    "avg_lens_df_1 = pd.DataFrame.from_dict([avg_lemma_lens_1, avg_word_lens_1]).transpose().rename(columns={0: 'All lemmas avg length', 1: 'All words avg length'})\n",
    "avg_lens_df_2 = pd.DataFrame.from_dict([avg_lemma_lens_2, avg_word_lens_2]).transpose().rename(columns={0: 'All lemmas avg length', 1: 'All words avg length'})\n",
    "avg_lens_df_3 = pd.DataFrame.from_dict([avg_lemma_lens_3, avg_word_lens_3]).transpose().rename(columns={0: 'All lemmas avg length', 1: 'All words avg length'})\n",
    "avg_lens_df = pd.DataFrame.from_dict([avg_lemma_lens, avg_word_lens]).transpose().rename(columns={0: 'Unique lemmas avg length', 1: 'Unique words avg length'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for getting values for different variables used in metrics\n",
    "\n",
    "#Function for getting the total length of the corpus\n",
    "def getL(word_amounts: dict) -> int:\n",
    "    l = 0\n",
    "    for key in word_amounts:\n",
    "        l += word_amounts[key]\n",
    "    return l\n",
    "#Function for getting how big each part is in relation to the total size of the corpus\n",
    "def getS(word_amounts: dict, l: int) -> dict:\n",
    "    s = {}\n",
    "    for key in word_amounts:\n",
    "        s[key] = (word_amounts[key]*1.0)/l\n",
    "    return s\n",
    "\n",
    "#Get the total frequencies of passed freq_data in the corpus\n",
    "def getTotal(freq_data: dict) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "    #Add all dataframes to list\n",
    "    for key in freq_data:\n",
    "        dfs.append(freq_data[key])\n",
    "    #Concat all dataframes together\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    #Return a dataframe containing text in one column and total freq in collection in the other\n",
    "    return df.groupby(df.columns[0])['frequency'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants to be used in different measures\n",
    "\n",
    "#The length of the corpus in words (no PUNCT)\n",
    "l_1 = getL(word_amounts_1)\n",
    "l_2 = getL(word_amounts_2)\n",
    "l_3 = getL(word_amounts_3)\n",
    "l = l_1+l_2+l_3\n",
    "#The length of the corpus in parts\n",
    "n = len(sentences.keys())\n",
    "#The percentages of the n corpus part sizes\n",
    "s_1 = getS(word_amounts_1, l_1)\n",
    "s_2 = getS(word_amounts_2, l_2)\n",
    "s_3 = getS(word_amounts_3, l_3)\n",
    "s = getS(word_amounts, l)\n",
    "#The overall frequencies of words in corpus\n",
    "f_words_1 = getTotal(word_freqs_1)\n",
    "f_words_2 = getTotal(word_freqs_2)\n",
    "f_words_3 = getTotal(word_freqs_3)\n",
    "f_words = getTotal(word_freqs)\n",
    "#The overall frequencies of lemmas in corpus\n",
    "f_lemmas_1 = getTotal(lemma_freqs_1)\n",
    "f_lemmas_2 = getTotal(lemma_freqs_2)\n",
    "f_lemmas_3 = getTotal(lemma_freqs_3)\n",
    "f_lemmas = getTotal(lemma_freqs)\n",
    "#The frequencies of words in each corpus part\n",
    "v_words = word_freqs\n",
    "#The frequencies of lemmas in each corpus part\n",
    "v_lemmas = lemma_freqs\n",
    "\n",
    "#print(f_words.sort_values(by='frequency', ascending=False))\n",
    "#print(f_lemmas.sort_values(by='frequency', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548ddac5084b4426be615cdbb980c078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DP calculations:   0%|          | 0/87030 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a35022ad1704e17b2285e707b4cde43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DP calculations:   0%|          | 0/9410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8807e85c91c4e459480a99215f08869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DP calculations:   0%|          | 0/38232 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332a9dd68e974ce5a762f3a9bd5cd399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DP calculations:   0%|          | 0/67062 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24339aedf2844aca01c2f7ce609a6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DP calculations:   0%|          | 0/202732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912d39541306498aac27080abad947a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DP calculations:   0%|          | 0/18650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19695b29fb3c494284f29cbe512cf951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DP calculations:   0%|          | 0/87211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f9f85f08e04b41852818d4dd68d208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DP calculations:   0%|          | 0/156177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Whole corpus\n",
    "lemma_DP = getDP(v_lemmas, f_lemmas, s)\n",
    "#Sub-corpora\n",
    "lemma_DP_1 = getDP(lemma_freqs_1, f_lemmas_1, s_1)\n",
    "lemma_DP_2 = getDP(lemma_freqs_2, f_lemmas_2, s_2)\n",
    "lemma_DP_3 = getDP(lemma_freqs_3, f_lemmas_3, s_3)\n",
    "#Whole corpus\n",
    "word_DP = getDP(v_words, f_words, s)\n",
    "#Sub-corpora\n",
    "word_DP_1 = getDP(word_freqs_1, f_words_1, s_1)\n",
    "word_DP_2 = getDP(word_freqs_2, f_words_2, s_2)\n",
    "word_DP_3 = getDP(word_freqs_3, f_words_3, s_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing outputs\n",
    "test = 0\n",
    "for key in s:\n",
    "    test += s[key]\n",
    "#print(test)\n",
    "\n",
    "#with pd.ExcelWriter(\"Data/lemma_DP.xlsx\") as writer:\n",
    "#    lemma_DP.to_excel(writer)\n",
    "\n",
    "#lemma_DP['frequency'] = f_lemmas['frequency']\n",
    "\n",
    "#lemma_DP = lemma_DP.drop(columns=['frequency'])\n",
    "\n",
    "#print(lemma_DP.sort_values(by='DP', ascending=True))\n",
    "#ax = lemma_DP.sort_values(by='DP_norm').plot.hist(bins=100)\n",
    "\n",
    "#print(len(lemma_DP[lemma_DP['DP_norm']>0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting CD\n",
    "\n",
    "#Whole corpus\n",
    "word_CD = getCD(v_words)\n",
    "#Sub-corpora\n",
    "word_CD_1 = getCD(word_freqs_1)\n",
    "word_CD_2 = getCD(word_freqs_2)\n",
    "word_CD_3 = getCD(word_freqs_3)\n",
    "\n",
    "#Whole corpus\n",
    "lemma_CD = getCD(v_lemmas)\n",
    "#Sub-corpora\n",
    "lemma_CD_1 = getCD(lemma_freqs_1)\n",
    "lemma_CD_2 = getCD(lemma_freqs_2)\n",
    "lemma_CD_3 = getCD(lemma_freqs_3)\n",
    "\n",
    "#print(word_CD)\n",
    "\n",
    "#print(lemma_CD[lemma_CD < 0.05].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get POS frequencies\n",
    "\n",
    "#Count POS frequencies\n",
    "\n",
    "pos_freqs_per_book = getPOSFrequencies(sentences)\n",
    "\n",
    "pos_freqs_1 = getTotal(getSubCorp(pos_freqs_per_book, 1))\n",
    "pos_freqs_2 = getTotal(getSubCorp(pos_freqs_per_book, 2))\n",
    "pos_freqs_3 = getTotal(getSubCorp(pos_freqs_per_book, 3))\n",
    "\n",
    "pos_freqs_corpus = getTotal(pos_freqs_per_book)\n",
    "\n",
    "#print(pos_freqs_corpus.sort_values(by='frequency', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing all data into one big xlsx-file\n",
    "def writeDataToXlsx(name, f_words, f_lemmas, pos_freqs, lemma_DP, word_DP, lemma_CD, word_CD, avg_uniq_lens_df, avg_lens_df):\n",
    "    with pd.ExcelWriter(\"Data/\"+name+\".xlsx\") as writer:\n",
    "        f_words.to_excel(writer, sheet_name=\"Word frequencies\")\n",
    "        f_lemmas.to_excel(writer, sheet_name=\"Lemma frequencies\")\n",
    "        pos_freqs.to_excel(writer, sheet_name=\"POS frequencies\")\n",
    "        lemma_DP.to_excel(writer, sheet_name=\"Lemma dispersion\")\n",
    "        word_DP.to_excel(writer, sheet_name=\"Word dispersion\")\n",
    "        lemma_CD.to_excel(writer, sheet_name=\"Lemma contextual diversity\")\n",
    "        word_CD.to_excel(writer, sheet_name=\"Word contextual diversity\")\n",
    "        avg_uniq_lens_df.to_excel(writer, sheet_name=\"Average unique lengths by book\")\n",
    "        avg_lens_df.to_excel(writer, sheet_name=\"Average lengths by book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commencing the writing part\n",
    "#writeDataToXlsx(\"Initial56_whole\", f_words, f_lemmas, pos_freqs_corpus, lemma_DP, word_DP, lemma_CD, word_CD, avg_uniq_lens_df, avg_lens_df)\n",
    "#writeDataToXlsx(\"Initial56_1\", f_words_1, f_lemmas_1, pos_freqs_1, lemma_DP_1, word_DP_1, lemma_CD_1, word_CD_1, avg_uniq_lens_df_1, avg_lens_df_1)\n",
    "#writeDataToXlsx(\"Initial56_2\", f_words_2, f_lemmas_2, pos_freqs_2, lemma_DP_2, word_DP_2, lemma_CD_2, word_CD_2, avg_uniq_lens_df_2, avg_lens_df_2)\n",
    "#writeDataToXlsx(\"Initial56_3\", f_words_3, f_lemmas_3, pos_freqs_3, lemma_DP_3, word_DP_3, lemma_CD_3, word_CD_3, avg_uniq_lens_df_3, avg_lens_df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'float' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m t \u001b[38;5;241m=\u001b[39m (f_lemmas\u001b[38;5;241m.\u001b[39mfrequency)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: math\u001b[38;5;241m.\u001b[39mlog10(x\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000000\u001b[39m))\n\u001b[1;32m     11\u001b[0m m,k \u001b[38;5;241m=\u001b[39m norm\u001b[38;5;241m.\u001b[39mfit(f_lemmas\u001b[38;5;241m.\u001b[39mfrequency)\n\u001b[0;32m---> 13\u001b[0m log_l \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(np\u001b[38;5;241m.\u001b[39mprod(\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_lemmas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrequency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(log_l)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#ax = plt.hist(lemma_DP.DP_norm, range=[0, 1])\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Test/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py:2046\u001b[0m, in \u001b[0;36mrv_continuous.pdf\u001b[0;34m(self, x, *args, **kwds)\u001b[0m\n\u001b[1;32m   2044\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(asarray, args))\n\u001b[1;32m   2045\u001b[0m dtyp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpromote_types(x\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m-> 2046\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mscale\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mdtyp)\n\u001b[1;32m   2047\u001b[0m cond0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_argcheck(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m&\u001b[39m (scale \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2048\u001b[0m cond1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_support_mask(x, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m&\u001b[39m (scale \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'float' and 'dict'"
     ]
    }
   ],
   "source": [
    "#Sorting out some differences and distributions\n",
    "\n",
    "# zipf = log10((*word_freq*/10000000+*1*)/(*total_token_count*/10000000 + *unique_words*/10000000))+3.0\n",
    "\n",
    "\n",
    "\n",
    "#print(f_lemmas)\n",
    "\n",
    "#t = (f_lemmas.frequency).apply(lambda x: math.log10(x/1000000))\n",
    "\n",
    "#m,k = norm.fit(f_lemmas.frequency)\n",
    "\n",
    "#log_l = np.log(np.prod(norm.pdf(f_lemmas.frequency, m, s)))\n",
    "\n",
    "#print(log_l)\n",
    "\n",
    "\n",
    "#ax = plt.hist(lemma_DP.DP_norm, range=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
