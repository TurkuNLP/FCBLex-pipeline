{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from scripts import bookdatafunctions as bdf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import bookdatafunctions as bdf\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Constants\n",
    "JSON_PATH = \"Parsed\"\n",
    "CONLLU_PATH = \"Conllus\"\n",
    "ISBN2AGE_PATH = \"ISBN_MAPS/ISBN2AGE.xlsx\"\n",
    "\n",
    "\n",
    "\n",
    "#books = bdf.initBooksFromJsons(JSON_PATH)\n",
    "\n",
    "#Move to working with just sentence data\n",
    "#Whole corpus\n",
    "corpus = bdf.mapGroup2Age(bdf.cleanWordBeginnings(bdf.initBooksFromConllus(CONLLU_PATH)), ISBN2AGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatDataForPaperOutput(corpus: dict[str,pd.DataFrame]):\n",
    "    ages = sorted(bdf.getAvailableAges(corpus))\n",
    "\n",
    "    ready_dfs_ages = {}\n",
    "    ready_dfs_groups = {}\n",
    "    ready_dfs_whole = {}\n",
    "\n",
    "    #Subcorpora based on the target age groups\n",
    "    sub_corpora = []\n",
    "    #Combine books aged 15 and up into one sub-corpus as there are very few entries in 16,17,18\n",
    "    over_15 = []\n",
    "    for i in ages:\n",
    "        if i<15:\n",
    "            sub_corpora.append(bdf.cleanWordBeginnings(bdf.getDistinctSubCorp(corpus, i)))\n",
    "        else:\n",
    "            over_15.append(bdf.cleanWordBeginnings(bdf.getDistinctSubCorp(corpus, i)))\n",
    "    #Sort the aged 15 and over sub-corpora from lowest age to highest\n",
    "    over_15.sort(key=lambda x:int(bdf.findAgeFromID(list(x.keys())[0])))\n",
    "    #Combine 15+ aged books into one sub-corpus\n",
    "    sub_corpora.append(bdf.combineSubCorpDicts(over_15))\n",
    "    #Sort the sub-corpora from lowest age to highest\n",
    "    sub_corpora.sort(key=lambda x:int(bdf.findAgeFromID(list(x.keys())[0])))\n",
    "    #Keep track of when words first appear in terms of intended reading age\n",
    "    word_age_appearances = {}\n",
    "    #First sort out the subcorpora\n",
    "    for sub_corp in sub_corpora:\n",
    "        combined_data = pd.concat(sub_corp.values()).reset_index()\n",
    "        filtered_data = combined_data[['text','lemma','upos']]\n",
    "        filtered_data = filtered_data.drop_duplicates(['text','lemma','upos'], ignore_index=True)\n",
    "        #Add word-pos frequencies\n",
    "        v_words_pos = bdf.getColumnFrequencies(sub_corp, ['text','upos'])\n",
    "        word_pos_freqs = bdf.combineFrequencies(v_words_pos)\n",
    "        filtered_data['Word-POS Frequency'] = [word_pos_freqs[x[0]][x[1]] for x in filtered_data[['text','upos']].to_numpy(dtype='str')]\n",
    "        #Add word frequencies\n",
    "        v_words = bdf.getColumnFrequencies(sub_corp, ['text'])\n",
    "        word_freqs = bdf.combineFrequencies(v_words)\n",
    "        filtered_data['Word Frequency'] = [word_freqs[x] for x in filtered_data['text'].to_numpy(dtype='str')]\n",
    "        #Add word zipf-values\n",
    "        l = bdf.getL(bdf.getTokenAmounts(sub_corp))\n",
    "        word_zipfs = bdf.getZipfValues(l, word_freqs)\n",
    "        filtered_data['Word Zipf'] = [word_zipfs[x] for x in filtered_data['text'].to_numpy(dtype='str')]\n",
    "        #Add word DP\n",
    "        word_DP = bdf.getDP(v_words, word_freqs, bdf.getS(bdf.getTokenAmounts(sub_corp), l))[0]\n",
    "        filtered_data['Word DP'] = [word_DP[x] for x in filtered_data['text'].to_numpy(dtype='str')]\n",
    "        #Add word CD\n",
    "        word_CD = bdf.getCD(v_words)\n",
    "        filtered_data['Word CD'] = [word_CD[x] for x in filtered_data['text'].to_numpy(dtype='str')]\n",
    "        #Add lemma frequencies\n",
    "        v_lemmas = bdf.getColumnFrequencies(sub_corp, ['lemma'])\n",
    "        lemma_freqs = bdf.combineFrequencies(v_lemmas)\n",
    "        filtered_data['Lemma Frequency'] = [lemma_freqs[x] for x in filtered_data['lemma'].to_numpy(dtype='str')]\n",
    "        #Add word zipf-values\n",
    "        lemma_zipfs = bdf.getZipfValues(l, lemma_freqs)\n",
    "        filtered_data['Lemma Zipf'] = [lemma_zipfs[x] for x in filtered_data['lemma'].to_numpy(dtype='str')]\n",
    "        #Add word DP\n",
    "        lemma_DP = bdf.getDP(v_lemmas, lemma_freqs, bdf.getS(bdf.getTokenAmounts(sub_corp), l))[0]\n",
    "        filtered_data['Lemma DP'] = [lemma_DP[x] for x in filtered_data['lemma'].to_numpy(dtype='str')]\n",
    "        #Add word CD\n",
    "        lemma_CD = bdf.getCD(v_lemmas)\n",
    "        filtered_data['Lemma CD'] = [lemma_CD[x] for x in filtered_data['lemma'].to_numpy(dtype='str')]\n",
    "        #Add taivutusperhe size\n",
    "        tv_sizes = bdf.getTaivutusperheSize(sub_corp)\n",
    "        filtered_data['Lemma inflection family size'] = [tv_sizes[x] for x in filtered_data['lemma'].to_numpy(dtype='str')]\n",
    "        key = bdf.findAgeFromID(list(sub_corp.keys())[0])\n",
    "        #Slow but steady way of adding words and first appearance ages...\n",
    "        for w in word_freqs.index:\n",
    "            word_age_appearances.setdefault(w[0],key)\n",
    "        #Add first appearance\n",
    "        filtered_data['First Age Encountered'] = [word_age_appearances[x] for x in filtered_data['text'].to_numpy(dtype='str')]\n",
    "        #Add to dictionary\n",
    "        ready_dfs_ages[key] = filtered_data.sort_values('text')\n",
    "    \n",
    "    #Define age group sub-corpora\n",
    "\n",
    "    \n",
    "    #Generate correct keys/ids\n",
    "    group_1 = [5,6,7,8]\n",
    "    group_2 = [9,10,11,12]\n",
    "    group_3 = ages[ages.index(13):]\n",
    "    #Distinct subcorpora\n",
    "    sub_corp_1= bdf.combineSubCorpDicts([bdf.getDistinctSubCorp(corpus, x) for x in group_1])\n",
    "    sub_corp_2= bdf.combineSubCorpDicts([bdf.getDistinctSubCorp(corpus, x) for x in group_2])\n",
    "    sub_corp_3= bdf.combineSubCorpDicts([bdf.getDistinctSubCorp(corpus, x) for x in group_3])\n",
    "    sub_corps = dict(zip(['7-8','9-12','13+'],[sub_corp_1, sub_corp_2, sub_corp_3]))\n",
    "\n",
    "    for s in sub_corps:\n",
    "        sub_corp = sub_corps[s]\n",
    "        combined_data = pd.concat(sub_corp.values()).reset_index()\n",
    "        filtered_data = combined_data[['text','lemma','upos']]\n",
    "        filtered_data = filtered_data.drop_duplicates(['text','lemma','upos'], ignore_index=True)\n",
    "        #Add word-pos frequencies\n",
    "        v_words_pos = bdf.getColumnFrequencies(sub_corp, ['text','upos'])\n",
    "        word_pos_freqs = bdf.combineFrequencies(v_words_pos)\n",
    "        filtered_data['Word-POS Frequency'] = [word_pos_freqs[x[0]][x[1]] for x in filtered_data[['text','upos']].to_numpy(dtype='str')]\n",
    "        #Add word frequencies\n",
    "        v_words = bdf.getColumnFrequencies(sub_corp, ['text'])\n",
    "        word_freqs = bdf.combineFrequencies(v_words)\n",
    "        filtered_data['Word Frequency'] = [word_freqs[x] for x in filtered_data['text'].to_numpy(dtype='str')]\n",
    "        #Add word zipf-values\n",
    "        l = bdf.getL(bdf.getTokenAmounts(sub_corp))\n",
    "        word_zipfs = bdf.getZipfValues(l, word_freqs)\n",
    "        filtered_data['Word Zipf'] = [word_zipfs[x] for x in filtered_data['text'].to_numpy(dtype='str')]\n",
    "        #Add word DP\n",
    "        word_DP = bdf.getDP(v_words, word_freqs, bdf.getS(bdf.getTokenAmounts(sub_corp), l))[0]\n",
    "        filtered_data['Word DP'] = [word_DP[x] for x in filtered_data['text'].to_numpy(dtype='str')]\n",
    "        #Add word CD\n",
    "        word_CD = bdf.getCD(v_words)\n",
    "        filtered_data['Word CD'] = [word_CD[x] for x in filtered_data['text'].to_numpy(dtype='str')]\n",
    "        #Add lemma frequencies\n",
    "        v_lemmas = bdf.getColumnFrequencies(sub_corp, ['lemma'])\n",
    "        lemma_freqs = bdf.combineFrequencies(v_lemmas)\n",
    "        filtered_data['Lemma Frequency'] = [lemma_freqs[x] for x in filtered_data['lemma'].to_numpy(dtype='str')]\n",
    "        #Add word zipf-values\n",
    "        lemma_zipfs = bdf.getZipfValues(l, lemma_freqs)\n",
    "        filtered_data['Lemma Zipf'] = [lemma_zipfs[x] for x in filtered_data['lemma'].to_numpy(dtype='str')]\n",
    "        #Add word DP\n",
    "        lemma_DP = bdf.getDP(v_lemmas, lemma_freqs, bdf.getS(bdf.getTokenAmounts(sub_corp), l))[0]\n",
    "        filtered_data['Lemma DP'] = [lemma_DP[x] for x in filtered_data['lemma'].to_numpy(dtype='str')]\n",
    "        #Add word CD\n",
    "        lemma_CD = bdf.getCD(v_lemmas)\n",
    "        filtered_data['Lemma CD'] = [lemma_CD[x] for x in filtered_data['lemma'].to_numpy(dtype='str')]\n",
    "        #Add taivutusperhe size\n",
    "        tv_sizes = bdf.getTaivutusperheSize(sub_corp)\n",
    "        filtered_data['Lemma inflection family size'] = [tv_sizes[x] for x in filtered_data['lemma'].to_numpy(dtype='str')]\n",
    "        key = s\n",
    "        #Add first appearance\n",
    "        filtered_data['First Age Encountered'] = [word_age_appearances[x] for x in filtered_data['text'].to_numpy(dtype='str')]\n",
    "        #Add to dictionary\n",
    "        ready_dfs_groups[key] = filtered_data.sort_values('text')\n",
    "\n",
    "    #Work with the whole corpus\n",
    "    combined_data = pd.concat(corpus.values()).reset_index()\n",
    "    filtered_data = combined_data[['text','lemma','upos']]\n",
    "    filtered_data = filtered_data.drop_duplicates(['text','lemma','upos'], ignore_index=True)\n",
    "    #Add word-pos frequencies\n",
    "    v_words_pos = bdf.getColumnFrequencies(corpus, ['text','upos'])\n",
    "    word_pos_freqs = bdf.combineFrequencies(v_words_pos)\n",
    "    filtered_data['Word-POS Frequency'] = [word_pos_freqs[x[0]][x[1]] for x in filtered_data[['text','upos']].to_numpy(dtype='str')]\n",
    "    #Add word frequencies\n",
    "    v_words = bdf.getColumnFrequencies(corpus, ['text'])\n",
    "    word_freqs = bdf.combineFrequencies(v_words)\n",
    "    filtered_data['Word Frequency'] = [word_freqs[x] for x in filtered_data['text'].to_numpy(dtype='str')]\n",
    "    #Add word zipf-values\n",
    "    l = bdf.getL(bdf.getTokenAmounts(corpus))\n",
    "    word_zipfs = bdf.getZipfValues(l, word_freqs)\n",
    "    filtered_data['Word Zipf'] = [word_zipfs[x] for x in filtered_data['text'].to_numpy(dtype='str')]\n",
    "    #Add word DP\n",
    "    word_DP = bdf.getDP(v_words, word_freqs, bdf.getS(bdf.getTokenAmounts(corpus), l))[0]\n",
    "    filtered_data['Word DP'] = [word_DP[x] for x in filtered_data['text'].to_numpy(dtype='str')]\n",
    "    #Add word CD\n",
    "    word_CD = bdf.getCD(v_words)\n",
    "    filtered_data['Word CD'] = [word_CD[x] for x in filtered_data['text'].to_numpy(dtype='str')]\n",
    "    #Add lemma frequencies\n",
    "    v_lemmas = bdf.getColumnFrequencies(corpus, ['lemma'])\n",
    "    lemma_freqs = bdf.combineFrequencies(v_lemmas)\n",
    "    filtered_data['Lemma Frequency'] = [lemma_freqs[x] for x in filtered_data['lemma'].to_numpy(dtype='str')]\n",
    "    #Add word zipf-values\n",
    "    lemma_zipfs = bdf.getZipfValues(l, lemma_freqs)\n",
    "    filtered_data['Lemma Zipf'] = [lemma_zipfs[x] for x in filtered_data['lemma'].to_numpy(dtype='str')]\n",
    "    #Add word DP\n",
    "    lemma_DP = bdf.getDP(v_lemmas, lemma_freqs, bdf.getS(bdf.getTokenAmounts(corpus), l))[0]\n",
    "    filtered_data['Lemma DP'] = [lemma_DP[x] for x in filtered_data['lemma'].to_numpy(dtype='str')]\n",
    "    #Add word CD\n",
    "    lemma_CD = bdf.getCD(v_lemmas)\n",
    "    filtered_data['Lemma CD'] = [lemma_CD[x] for x in filtered_data['lemma'].to_numpy(dtype='str')]\n",
    "    #Add taivutusperhe size\n",
    "    tv_sizes = bdf.getTaivutusperheSize(corpus)\n",
    "    filtered_data['Lemma inflection family size'] = [tv_sizes[x] for x in filtered_data['lemma'].to_numpy(dtype='str')]\n",
    "    #Add first appearance\n",
    "    filtered_data['First Age Encountered'] = [word_age_appearances[x] for x in filtered_data['text'].to_numpy(dtype='str')]\n",
    "    #Add to dictionary\n",
    "    ready_dfs_whole['Whole'] = filtered_data.sort_values('text')\n",
    "\n",
    "    return ready_dfs_ages, ready_dfs_groups, ready_dfs_whole\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePaperOutputAges(ready_dfs: dict[str:pd.DataFrame], name: str):\n",
    "    with pd.ExcelWriter(\"Data/FCBLex_data_output_\"+name+\".xlsx\") as writer:\n",
    "        for df in ready_dfs:\n",
    "            ready_dfs[df].to_excel(writer, sheet_name=df, index=False)\n",
    "            print(df+\" done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DP calculations:  62%|██████▏   | 75104/121821 [00:09<00:06, 7547.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dfs \u001b[38;5;241m=\u001b[39m \u001b[43mformatDataForPaperOutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m, in \u001b[0;36mformatDataForPaperOutput\u001b[0;34m(corpus)\u001b[0m\n\u001b[1;32m     37\u001b[0m filtered_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord Zipf\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [word_zipfs[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m filtered_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#Add word DP\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m word_DP \u001b[38;5;241m=\u001b[39m \u001b[43mbdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_freqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetTokenAmounts\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_corp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     40\u001b[0m filtered_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord DP\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [word_DP[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m filtered_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#Add word CD\u001b[39;00m\n",
      "File \u001b[0;32m~/Codings/FCBLex-pipeline/scripts/bookdatafunctions.py:277\u001b[0m, in \u001b[0;36mgetDP\u001b[0;34m(v, f_series, s)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(f_series)), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDP calculations\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;66;03m#Loop through every single word in the corpus\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m#Get the freq of the word in the whole corpus\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         f \u001b[38;5;241m=\u001b[39m \u001b[43mf_series\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m#Calculations according to the dispersion measure by Gries 2020\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         abs_sum_i \u001b[38;5;241m=\u001b[39m [((v[key]\u001b[38;5;241m.\u001b[39mget(word, \u001b[38;5;241m0.0\u001b[39m))\u001b[38;5;241m/\u001b[39mf)\u001b[38;5;241m-\u001b[39ms[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m v]\n",
      "File \u001b[0;32m~/miniconda3/envs/Test/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/miniconda3/envs/Test/lib/python3.10/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/miniconda3/envs/Test/lib/python3.10/site-packages/pandas/core/indexes/multi.py:3040\u001b[0m, in \u001b[0;36mMultiIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n\u001b[1;32m   3039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m-> 3040\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_level_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _maybe_to_slice(loc)\n\u001b[1;32m   3043\u001b[0m keylen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(key)\n",
      "File \u001b[0;32m~/miniconda3/envs/Test/lib/python3.10/site-packages/pandas/core/indexes/multi.py:3414\u001b[0m, in \u001b[0;36mMultiIndex._get_level_indexer\u001b[0;34m(self, key, level, indexer)\u001b[0m\n\u001b[1;32m   3412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3413\u001b[0m     start \u001b[38;5;241m=\u001b[39m algos\u001b[38;5;241m.\u001b[39msearchsorted(level_codes, idx, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3414\u001b[0m     end \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel_codes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mright\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;241m==\u001b[39m end:\n\u001b[1;32m   3417\u001b[0m     \u001b[38;5;66;03m# The label is present in self.levels[level] but unused:\u001b[39;00m\n\u001b[1;32m   3418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "File \u001b[0;32m~/miniconda3/envs/Test/lib/python3.10/site-packages/pandas/core/algorithms.py:1308\u001b[0m, in \u001b[0;36msearchsorted\u001b[0;34m(arr, value, side, sorter)\u001b[0m\n\u001b[1;32m   1297\u001b[0m     sorter \u001b[38;5;241m=\u001b[39m ensure_platform_int(sorter)\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(arr, np\u001b[38;5;241m.\u001b[39mndarray)\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;66;03m# Before searching below, we therefore try to give `value` the\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;66;03m# same dtype as `arr`, while guarding against integer overflows.\u001b[39;00m\n\u001b[0;32m-> 1308\u001b[0m     iinfo \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1309\u001b[0m     value_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([value]) \u001b[38;5;28;01mif\u001b[39;00m is_integer(value) \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(value)\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (value_arr \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m iinfo\u001b[38;5;241m.\u001b[39mmin)\u001b[38;5;241m.\u001b[39mall() \u001b[38;5;129;01mand\u001b[39;00m (value_arr \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m iinfo\u001b[38;5;241m.\u001b[39mmax)\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;66;03m# value within bounds, so no overflow, so can convert value dtype\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m         \u001b[38;5;66;03m# to dtype of arr\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Test/lib/python3.10/site-packages/numpy/core/getlimits.py:693\u001b[0m, in \u001b[0;36miinfo.__init__\u001b[0;34m(self, int_type)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkind, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbits)\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid integer data type \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkind,))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dfs_ages, dfs_groups, dfs_whole = formatDataForPaperOutput(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writePaperOutput(dfs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
